{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dying-james",
   "metadata": {},
   "source": [
    "# Consumer Behavior Analytics - Data Modelling  of `customers_whole`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-webcam",
   "metadata": {},
   "source": [
    "**Libraries and imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DS libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DataViz libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics Libraries\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "# Data Utils\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, \\\n",
    "                                                                     recall_score, \\\n",
    "                                                                     precision_score, \\\n",
    "                                                                     accuracy_score, \\\n",
    "                                                                     roc_auc_score, \\\n",
    "                                                                     auc, \\\n",
    "                                                                     plot_confusion_matrix, \\\n",
    "                                                                     plot_roc_curve\n",
    "                                                                         \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Notebook setup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occupied-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading customers exposed\n",
    "customers_exposed = pd.read_csv('../data/customers_exposed.csv', parse_dates = ['Dt_Customer'])\n",
    "\n",
    "# Loading customers whole\n",
    "customers_whole = pd.read_csv('../data/customers_whole.csv', parse_dates = ['Dt_Customer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "residential-colleague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Dt_Customer</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>MntGoldProds</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>AcceptedCmp3</th>\n",
       "      <th>AcceptedCmp4</th>\n",
       "      <th>AcceptedCmp5</th>\n",
       "      <th>AcceptedCmp1</th>\n",
       "      <th>AcceptedCmp2</th>\n",
       "      <th>Complain</th>\n",
       "      <th>Response</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Income_PerCap</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Prop_Spending_Income_pc</th>\n",
       "      <th>Total_Puchases</th>\n",
       "      <th>Avg_Ticket</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5524</td>\n",
       "      <td>1957</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>1617</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>25</td>\n",
       "      <td>64.68</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2174</td>\n",
       "      <td>1954</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-03-08</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15448.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>6</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4141</td>\n",
       "      <td>1965</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-21</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35806.5</td>\n",
       "      <td>776</td>\n",
       "      <td>0.021672</td>\n",
       "      <td>21</td>\n",
       "      <td>36.95</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6182</td>\n",
       "      <td>1984</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Together</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8882.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>8</td>\n",
       "      <td>6.62</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5324</td>\n",
       "      <td>1981</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-19</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19431.0</td>\n",
       "      <td>422</td>\n",
       "      <td>0.021718</td>\n",
       "      <td>19</td>\n",
       "      <td>22.21</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n",
       "0  5524        1957  Graduation         Single  58138.0        0         0   \n",
       "1  2174        1954  Graduation         Single  46344.0        1         1   \n",
       "2  4141        1965  Graduation       Together  71613.0        0         0   \n",
       "3  6182        1984  Graduation       Together  26646.0        1         0   \n",
       "4  5324        1981         PhD        Married  58293.0        1         0   \n",
       "\n",
       "  Dt_Customer  Recency  MntWines  MntFruits  MntMeatProducts  MntFishProducts  \\\n",
       "0  2012-09-04       58       635         88              546              172   \n",
       "1  2014-03-08       38        11          1                6                2   \n",
       "2  2013-08-21       26       426         49              127              111   \n",
       "3  2014-02-10       26        11          4               20               10   \n",
       "4  2014-01-19       94       173         43              118               46   \n",
       "\n",
       "   MntSweetProducts  MntGoldProds  NumDealsPurchases  NumWebPurchases  \\\n",
       "0                88            88                  3                8   \n",
       "1                 1             6                  2                1   \n",
       "2                21            42                  1                8   \n",
       "3                 3             5                  2                2   \n",
       "4                27            15                  5                5   \n",
       "\n",
       "   NumCatalogPurchases  NumStorePurchases  NumWebVisitsMonth  AcceptedCmp3  \\\n",
       "0                   10                  4                  7             0   \n",
       "1                    1                  2                  5             0   \n",
       "2                    2                 10                  4             0   \n",
       "3                    0                  4                  6             0   \n",
       "4                    3                  6                  5             0   \n",
       "\n",
       "   AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  Complain  Response  \\\n",
       "0             0             0             0             0         0         1   \n",
       "1             0             0             0             0         0         0   \n",
       "2             0             0             0             0         0         0   \n",
       "3             0             0             0             0         0         0   \n",
       "4             0             0             0             0         0         0   \n",
       "\n",
       "   Family_Size  Income_PerCap  Total_Spent  Prop_Spending_Income_pc  \\\n",
       "0            1        58138.0         1617                 0.027813   \n",
       "1            3        15448.0           27                 0.001748   \n",
       "2            2        35806.5          776                 0.021672   \n",
       "3            3         8882.0           53                 0.005967   \n",
       "4            3        19431.0          422                 0.021718   \n",
       "\n",
       "   Total_Puchases  Avg_Ticket  Age  \n",
       "0              25       64.68   57  \n",
       "1               6        4.50   60  \n",
       "2              21       36.95   49  \n",
       "3               8        6.62   30  \n",
       "4              19       22.21   33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinguished-murder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.850873\n",
       "1    0.149127\n",
       "Name: Response, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class balance (or imballance)\n",
    "customers_whole['Response'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-escape",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-cardiff",
   "metadata": {},
   "source": [
    "We are going to start preparing the data for modelling regarding both datasets: `customers_whole` and `customers_exposed`, but only until One Hot Encoding.\n",
    "\n",
    "After that, we will save both one hot encoded dataframes into new csv files and split both analysis in different notebooks. The analysis in this notebook will be for `customers_whole` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "light-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Sample first_date\n",
    "first_date = customers_whole['Dt_Customer'].min()\n",
    "\n",
    "# Transforming datetime feature to numeric feature\n",
    "for df in [customers_exposed, customers_whole]:\n",
    "    df['Dt_Customer_InDays'] = df['Dt_Customer'] - first_date\n",
    "    \n",
    "    df['Dt_Customer_InDays'] = (df['Dt_Customer_InDays'] / np.timedelta64(1, 'D')).astype(int) + 1\n",
    "    \n",
    "    # Dropping unuseful columns for modelling\n",
    "    df.drop(['ID', 'Dt_Customer'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "second-postcard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graduation    1126\n",
       "PhD            483\n",
       "Master         369\n",
       "2n Cycle       201\n",
       "Basic           54\n",
       "Name: Education, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_whole['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinct-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Education Variable\n",
    "# customers_whole['Education'] = customers_whole['Education'].map({'Basic': 1,\n",
    "#                                                                  '2n Cycle': 2,\n",
    "#                                                                  'Graduation': 3,\n",
    "#                                                                  'Master': 4, \n",
    "#                                                                  'PhD': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Kidhome</th>\n",
       "      <th>Teenhome</th>\n",
       "      <th>Recency</th>\n",
       "      <th>MntWines</th>\n",
       "      <th>MntFruits</th>\n",
       "      <th>MntMeatProducts</th>\n",
       "      <th>MntFishProducts</th>\n",
       "      <th>MntSweetProducts</th>\n",
       "      <th>MntGoldProds</th>\n",
       "      <th>NumDealsPurchases</th>\n",
       "      <th>NumWebPurchases</th>\n",
       "      <th>NumCatalogPurchases</th>\n",
       "      <th>NumStorePurchases</th>\n",
       "      <th>NumWebVisitsMonth</th>\n",
       "      <th>AcceptedCmp3</th>\n",
       "      <th>AcceptedCmp4</th>\n",
       "      <th>AcceptedCmp5</th>\n",
       "      <th>AcceptedCmp1</th>\n",
       "      <th>AcceptedCmp2</th>\n",
       "      <th>Complain</th>\n",
       "      <th>Response</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Income_PerCap</th>\n",
       "      <th>Total_Spent</th>\n",
       "      <th>Prop_Spending_Income_pc</th>\n",
       "      <th>Total_Puchases</th>\n",
       "      <th>Avg_Ticket</th>\n",
       "      <th>Age</th>\n",
       "      <th>Dt_Customer_InDays</th>\n",
       "      <th>Education_2n Cycle</th>\n",
       "      <th>Education_Basic</th>\n",
       "      <th>Education_Graduation</th>\n",
       "      <th>Education_Master</th>\n",
       "      <th>Education_PhD</th>\n",
       "      <th>Marital_Status_Divorced</th>\n",
       "      <th>Marital_Status_Married</th>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <th>Marital_Status_Together</th>\n",
       "      <th>Marital_Status_Widow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1957</td>\n",
       "      <td>58138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>635</td>\n",
       "      <td>88</td>\n",
       "      <td>546</td>\n",
       "      <td>172</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58138.00</td>\n",
       "      <td>1617</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>25</td>\n",
       "      <td>64.68</td>\n",
       "      <td>57</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1954</td>\n",
       "      <td>46344.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15448.00</td>\n",
       "      <td>27</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>6</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60</td>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1965</td>\n",
       "      <td>71613.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>426</td>\n",
       "      <td>49</td>\n",
       "      <td>127</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35806.50</td>\n",
       "      <td>776</td>\n",
       "      <td>0.021672</td>\n",
       "      <td>21</td>\n",
       "      <td>36.95</td>\n",
       "      <td>49</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>26646.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8882.00</td>\n",
       "      <td>53</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>8</td>\n",
       "      <td>6.62</td>\n",
       "      <td>30</td>\n",
       "      <td>561</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981</td>\n",
       "      <td>58293.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>173</td>\n",
       "      <td>43</td>\n",
       "      <td>118</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19431.00</td>\n",
       "      <td>422</td>\n",
       "      <td>0.021718</td>\n",
       "      <td>19</td>\n",
       "      <td>22.21</td>\n",
       "      <td>33</td>\n",
       "      <td>539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>1967</td>\n",
       "      <td>61223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>709</td>\n",
       "      <td>43</td>\n",
       "      <td>182</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>247</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20407.67</td>\n",
       "      <td>1341</td>\n",
       "      <td>0.065711</td>\n",
       "      <td>18</td>\n",
       "      <td>74.50</td>\n",
       "      <td>47</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>1946</td>\n",
       "      <td>64014.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12802.80</td>\n",
       "      <td>444</td>\n",
       "      <td>0.034680</td>\n",
       "      <td>22</td>\n",
       "      <td>20.18</td>\n",
       "      <td>68</td>\n",
       "      <td>681</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>1981</td>\n",
       "      <td>56981.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>908</td>\n",
       "      <td>48</td>\n",
       "      <td>217</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56981.00</td>\n",
       "      <td>1241</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>19</td>\n",
       "      <td>65.32</td>\n",
       "      <td>33</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>1956</td>\n",
       "      <td>69245.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>428</td>\n",
       "      <td>30</td>\n",
       "      <td>214</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23081.67</td>\n",
       "      <td>843</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>23</td>\n",
       "      <td>36.65</td>\n",
       "      <td>58</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>1954</td>\n",
       "      <td>52869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13217.25</td>\n",
       "      <td>172</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>11</td>\n",
       "      <td>15.64</td>\n",
       "      <td>60</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2233 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year_Birth   Income  Kidhome  Teenhome  Recency  MntWines  MntFruits  \\\n",
       "0           1957  58138.0        0         0       58       635         88   \n",
       "1           1954  46344.0        1         1       38        11          1   \n",
       "2           1965  71613.0        0         0       26       426         49   \n",
       "3           1984  26646.0        1         0       26        11          4   \n",
       "4           1981  58293.0        1         0       94       173         43   \n",
       "...          ...      ...      ...       ...      ...       ...        ...   \n",
       "2228        1967  61223.0        0         1       46       709         43   \n",
       "2229        1946  64014.0        2         1       56       406          0   \n",
       "2230        1981  56981.0        0         0       91       908         48   \n",
       "2231        1956  69245.0        0         1        8       428         30   \n",
       "2232        1954  52869.0        1         1       40        84          3   \n",
       "\n",
       "      MntMeatProducts  MntFishProducts  MntSweetProducts  MntGoldProds  \\\n",
       "0                 546              172                88            88   \n",
       "1                   6                2                 1             6   \n",
       "2                 127              111                21            42   \n",
       "3                  20               10                 3             5   \n",
       "4                 118               46                27            15   \n",
       "...               ...              ...               ...           ...   \n",
       "2228              182               42               118           247   \n",
       "2229               30                0                 0             8   \n",
       "2230              217               32                12            24   \n",
       "2231              214               80                30            61   \n",
       "2232               61                2                 1            21   \n",
       "\n",
       "      NumDealsPurchases  NumWebPurchases  NumCatalogPurchases  \\\n",
       "0                     3                8                   10   \n",
       "1                     2                1                    1   \n",
       "2                     1                8                    2   \n",
       "3                     2                2                    0   \n",
       "4                     5                5                    3   \n",
       "...                 ...              ...                  ...   \n",
       "2228                  2                9                    3   \n",
       "2229                  7                8                    2   \n",
       "2230                  1                2                    3   \n",
       "2231                  2                6                    5   \n",
       "2232                  3                3                    1   \n",
       "\n",
       "      NumStorePurchases  NumWebVisitsMonth  AcceptedCmp3  AcceptedCmp4  \\\n",
       "0                     4                  7             0             0   \n",
       "1                     2                  5             0             0   \n",
       "2                    10                  4             0             0   \n",
       "3                     4                  6             0             0   \n",
       "4                     6                  5             0             0   \n",
       "...                 ...                ...           ...           ...   \n",
       "2228                  4                  5             0             0   \n",
       "2229                  5                  7             0             0   \n",
       "2230                 13                  6             0             1   \n",
       "2231                 10                  3             0             0   \n",
       "2232                  4                  7             0             0   \n",
       "\n",
       "      AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  Complain  Response  \\\n",
       "0                0             0             0         0         1   \n",
       "1                0             0             0         0         0   \n",
       "2                0             0             0         0         0   \n",
       "3                0             0             0         0         0   \n",
       "4                0             0             0         0         0   \n",
       "...            ...           ...           ...       ...       ...   \n",
       "2228             0             0             0         0         0   \n",
       "2229             0             1             0         0         0   \n",
       "2230             0             0             0         0         0   \n",
       "2231             0             0             0         0         0   \n",
       "2232             0             0             0         0         1   \n",
       "\n",
       "      Family_Size  Income_PerCap  Total_Spent  Prop_Spending_Income_pc  \\\n",
       "0               1       58138.00         1617                 0.027813   \n",
       "1               3       15448.00           27                 0.001748   \n",
       "2               2       35806.50          776                 0.021672   \n",
       "3               3        8882.00           53                 0.005967   \n",
       "4               3       19431.00          422                 0.021718   \n",
       "...           ...            ...          ...                      ...   \n",
       "2228            3       20407.67         1341                 0.065711   \n",
       "2229            5       12802.80          444                 0.034680   \n",
       "2230            1       56981.00         1241                 0.021779   \n",
       "2231            3       23081.67          843                 0.036522   \n",
       "2232            4       13217.25          172                 0.013013   \n",
       "\n",
       "      Total_Puchases  Avg_Ticket  Age  Dt_Customer_InDays  Education_2n Cycle  \\\n",
       "0                 25       64.68   57                  37                   0   \n",
       "1                  6        4.50   60                 587                   0   \n",
       "2                 21       36.95   49                 388                   0   \n",
       "3                  8        6.62   30                 561                   0   \n",
       "4                 19       22.21   33                 539                   0   \n",
       "...              ...         ...  ...                 ...                 ...   \n",
       "2228              18       74.50   47                 319                   0   \n",
       "2229              22       20.18   68                 681                   0   \n",
       "2230              19       65.32   33                 545                   0   \n",
       "2231              23       36.65   58                 544                   0   \n",
       "2232              11       15.64   60                  78                   0   \n",
       "\n",
       "      Education_Basic  Education_Graduation  Education_Master  Education_PhD  \\\n",
       "0                   0                     1                 0              0   \n",
       "1                   0                     1                 0              0   \n",
       "2                   0                     1                 0              0   \n",
       "3                   0                     1                 0              0   \n",
       "4                   0                     0                 0              1   \n",
       "...               ...                   ...               ...            ...   \n",
       "2228                0                     1                 0              0   \n",
       "2229                0                     0                 0              1   \n",
       "2230                0                     1                 0              0   \n",
       "2231                0                     0                 1              0   \n",
       "2232                0                     0                 0              1   \n",
       "\n",
       "      Marital_Status_Divorced  Marital_Status_Married  Marital_Status_Single  \\\n",
       "0                           0                       0                      1   \n",
       "1                           0                       0                      1   \n",
       "2                           0                       0                      0   \n",
       "3                           0                       0                      0   \n",
       "4                           0                       1                      0   \n",
       "...                       ...                     ...                    ...   \n",
       "2228                        0                       1                      0   \n",
       "2229                        0                       0                      0   \n",
       "2230                        1                       0                      0   \n",
       "2231                        0                       0                      0   \n",
       "2232                        0                       1                      0   \n",
       "\n",
       "      Marital_Status_Together  Marital_Status_Widow  \n",
       "0                           0                     0  \n",
       "1                           0                     0  \n",
       "2                           1                     0  \n",
       "3                           1                     0  \n",
       "4                           0                     0  \n",
       "...                       ...                   ...  \n",
       "2228                        0                     0  \n",
       "2229                        1                     0  \n",
       "2230                        0                     0  \n",
       "2231                        1                     0  \n",
       "2232                        0                     0  \n",
       "\n",
       "[2233 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encoding categorical feature 'Marital_Status' with pd.get_dummies\n",
    "customers_exposed_ohe = pd.get_dummies(customers_exposed)\n",
    "customers_whole_ohe = pd.get_dummies(customers_whole)\n",
    "\n",
    "customers_whole_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "august-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving One Hot Enconded files into a new csv file\n",
    "customers_whole_ohe.to_csv('../data/customers_whole_ohe.csv', header = True, index = False)\n",
    "# pd.read_csv('../data/customers_whole_ohe.csv')\n",
    "\n",
    "customers_exposed_ohe.to_csv('../data/customers_exposed_ohe.csv', header = True, index = False)\n",
    "# pd.read_csv('../data/customers_exposed_ohe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-tribune",
   "metadata": {},
   "source": [
    "Both files have been saved! We will not need to load the `customers_whole_ohe.csv` into this notebook, but it is aways good to keep a standartd log of actions.\n",
    "\n",
    "**We will move forward with modelling for the `customers_whole` dataset hereafter.**\n",
    "\n",
    "Let's start:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-billion",
   "metadata": {},
   "source": [
    "### Splitting Data into _Train_, _Validation_ and _Test_ sets\n",
    "\n",
    "We will split the data according to the following schedule:\n",
    "\n",
    "- Create a `df_train` and a `df_test`.\n",
    "- From the previous `df_train` we will once again split it into two: `df_train` and `df_val`.\n",
    "\n",
    "We also know that _specially_ in this dataset (`_whole`) we have unballanced data. So we will perform a oversampling technique called SMOTE. According to the paper published in _The Journal of Artificial Intelligence Research_ in 2002:\n",
    "\n",
    "> [With SMOTE] The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the $k$ minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen.[$^{SMOTE: \\: Synthetic\\:Minority\\:Over-sampling\\:Technique}$](https://arxiv.org/pdf/1106.1813.pdf)\n",
    "\n",
    "- Finally, we will separate all dfs into `X`'s and `y`, naming respectively accordint to the df they belong to.\n",
    "\n",
    "Let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "certified-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df_train and df_test for training and testing\n",
    "df_train, df_test = train_test_split(customers_whole_ohe, test_size = .2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df_train into df_train and df_val\n",
    "df_train, df_val = train_test_split(df_train, test_size = .2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "miniature-replacement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1217\n",
       "1     211\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking target variable balance (or imballance)\n",
    "df_train['Response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surface-native",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1217\n",
       "1    1217\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balancing target variable with SMOTE technique\n",
    "\n",
    "# Instantiating SMOTER over_sampler\n",
    "smote = SMOTE(random_state = 7)\n",
    "\n",
    "# Fitting and resampling data with SMOTE\n",
    "X_train, y_train = smote.fit_resample(df_train.drop('Response', axis = 1), df_train['Response'])\n",
    "\n",
    "# Checking target class balance\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pressed-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val.drop('Response', axis = 1)\n",
    "y_val = df_val['Response']\n",
    "\n",
    "X_test = df_test.drop('Response', axis = 1)\n",
    "y_test = df_test['Response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-strength",
   "metadata": {},
   "source": [
    "Let's check if the generated `X`'s and `y`'s are correctly built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dimensional-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, y_train   shapes:  (2434, 40) (2434,)\n",
      "X_val  , y_val     shapes:  (358, 40) (358,)\n",
      "X_test , y_test    shapes:  (447, 40) (447,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train, y_train   shapes: ', X_train.shape, y_train.shape)\n",
    "print('X_val  , y_val     shapes: ', X_val.shape, y_val.shape)\n",
    "print('X_test , y_test    shapes: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-chicken",
   "metadata": {},
   "source": [
    "**All shapes match**. We are good to go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "continent-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model2 = RandomForestClassifier()\n",
    "param_grid = {\n",
    "            'n_estimators': [50, 60, 70, 80, 90, 100],\n",
    "            'max_depth': np.arange(0, 10),\n",
    "            'min_samples_split': np.arange(0, 10),\n",
    "            'min_samples_leaf': stats.loguniform(.01, 1),\n",
    "        }\n",
    "rf_model_rsearch = RandomizedSearchCV(rf_model2, param_distributions = param_grid, cv = 10, \n",
    "                                      n_jobs = -1, scoring = 'f1', random_state = 7, n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "defensive-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x12fcbc7c0>,\n",
       "                                        'min_samples_split': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'n_estimators': [50, 60, 70, 80, 90,\n",
       "                                                         100]},\n",
       "                   random_state=7, scoring='f1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_rsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "august-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_rsearch.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "friendly-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91       299\n",
      "           1       0.53      0.66      0.59        59\n",
      "\n",
      "    accuracy                           0.85       358\n",
      "   macro avg       0.73      0.77      0.75       358\n",
      "weighted avg       0.86      0.85      0.85       358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, rf_model_rsearch.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-canberra",
   "metadata": {},
   "source": [
    "### Analyzing multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-trick",
   "metadata": {},
   "source": [
    "In the previous notebooks, we have created new variables from pre-existing variables. Therefore we made room for possible multicolinearity.\n",
    "\n",
    "Some techniques for analyzing multicolinearity are:\n",
    "\n",
    "- Checking correlation values between variables;\n",
    "- Checking the Variance Inflation Factor (VIF) and dropping variables with factor $> 10$;\n",
    "- Performing Principal Component Analysis, to the cost of lesser interpretability;\n",
    "- Perform regularization such as (Lasso or Ridge) for linear models, such as Logistic Regression;\n",
    "\n",
    "For the sake of simplicity, let's go foward with `VIF` and drop variables with factor $ >10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-howard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numeric features in a list, except booleans\n",
    "numeric_features = [\n",
    "    'Year_Birth', \n",
    "#     'Education',\n",
    "    'Income', \n",
    "    'Kidhome',                \n",
    "    'Teenhome', \n",
    "    'Recency', \n",
    "    'MntWines', \n",
    "    'MntFruits',\n",
    "    'MntMeatProducts', \n",
    "    'MntFishProducts', \n",
    "    'MntSweetProducts',\n",
    "    'MntGoldProds', \n",
    "    'NumDealsPurchases', \n",
    "    'NumWebPurchases',\n",
    "    'NumCatalogPurchases', \n",
    "    'NumStorePurchases', \n",
    "    'NumWebVisitsMonth',\n",
    "    'Total_Spent',\n",
    "    'Total_Puchases',\n",
    "    'Family_Size',\n",
    "    'Income_PerCap',\n",
    "    'Prop_Spending_Income_pc', \n",
    "    'Avg_Ticket', \n",
    "    'Age', \n",
    "    'Dt_Customer_InDays'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of varibles for VIF analysis\n",
    "\n",
    "################################################################################################\n",
    "#   This cell has been iterated \"mannually\" after checking vif values in the dataframe below   #\n",
    "################################################################################################\n",
    "\n",
    "numeric_features_vif_ok = [\n",
    "#     'Year_Birth', \n",
    "#     'Education',\n",
    "#     'Income', \n",
    "    'Kidhome',                \n",
    "    'Teenhome', \n",
    "    'Recency', \n",
    "    'MntWines', \n",
    "    'MntFruits',\n",
    "    'MntMeatProducts', \n",
    "    'MntFishProducts', \n",
    "    'MntSweetProducts',\n",
    "    'MntGoldProds', \n",
    "    'NumDealsPurchases', \n",
    "    'NumWebPurchases',\n",
    "    'NumCatalogPurchases', \n",
    "    'NumStorePurchases', \n",
    "#     'NumWebVisitsMonth',\n",
    "#     'Total_Spent',\n",
    "#     'Total_Puchases',\n",
    "#     'Family_Size',\n",
    "    'Income_PerCap',\n",
    "    'Prop_Spending_Income_pc', \n",
    "    'Avg_Ticket', \n",
    "#     'Age', \n",
    "    'Dt_Customer_InDays'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-trunk",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for storing vif and its respective variable\n",
    "vif_df = pd.DataFrame()\n",
    "\n",
    "# Calculating vif values and saving it into vif_index columns\n",
    "vif_df[\"vif_index\"] = [vif(X_train[numeric_features_vif_ok].values, i) \\\n",
    "                               for i in range(X_train[numeric_features_vif_ok].shape[1])]\n",
    "\n",
    "# Saving variable name into feature column\n",
    "vif_df[\"feature\"] = X_train[numeric_features_vif_ok].columns\n",
    "\n",
    "# Checking results\n",
    "vif_df\n",
    "# del(vif_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-western",
   "metadata": {},
   "source": [
    "All `VIF` factor are now $\\le 10$, we can start dealing with the different orders of magnitude in our numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_from_vif = [feature for feature in numeric_features if feature not in numeric_features_vif_ok]\n",
    "to_drop_from_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-baker",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "In order to have the numeric data in the same order of magnite, we will:\n",
    "\n",
    "- Use RobustScaler for variables with outliers;\n",
    "- Use StandardScaler for variables with no outliers;\n",
    "\n",
    "\n",
    "Let's start by listing features with outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-lingerie",
   "metadata": {},
   "source": [
    "**Getting features with ouliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing features names if feature has outlier\n",
    "to_robust_scale = []\n",
    "for feature in numeric_features:\n",
    "    \n",
    "    Q1 = np.percentile(X_train[feature].sort_values(), 25, interpolation = 'midpoint')  \n",
    "    Q3 = np.percentile(X_train[feature].sort_values(), 75, interpolation = 'midpoint')  \n",
    "\n",
    "    IQR = Q3 - Q1  \n",
    "    \n",
    "    low_lim = Q1 - 1.5 * IQR \n",
    "    up_lim = Q3 + 1.5 * IQR \n",
    "\n",
    "    if (X_train[feature] > up_lim).any() or (X_train[feature] < low_lim).any(): \n",
    "         to_robust_scale.append(feature)\n",
    "\n",
    "to_robust_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-mouse",
   "metadata": {},
   "source": [
    "And then, from the previous list, we can list the variables that will be standardized:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-waterproof",
   "metadata": {},
   "source": [
    "**Listing features _without_ outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_standardize = [feature for feature in numeric_features if feature not in to_robust_scale]\n",
    "to_standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-office",
   "metadata": {},
   "source": [
    "**Applying RobustScaler to variables listed in `to_robust_scale` list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_train[to_robust_scale])\n",
    "X_train[to_robust_scale] = robust_scaler.transform(X_train[to_robust_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-science",
   "metadata": {},
   "source": [
    "**Applying StandardScaler to variables listed in `to_standardize` list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_scaler = StandardScaler()\n",
    "\n",
    "stand_scaler.fit(X_train[to_standardize])\n",
    "X_train[to_standardize] = stand_scaler.transform(X_train[to_standardize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-technical",
   "metadata": {},
   "source": [
    "Let's check the `X_train` dataset to see if everything went well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistics from scaled DFs\n",
    "round(X_train.describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-notification",
   "metadata": {},
   "source": [
    "The dataset seems alright.\n",
    "\n",
    "Aiming to avoid **data leakage**, we've performed the `.fit` method using only the `X_train` dataset. We need now to `.transform` the values from `X_val` and `X_test` datasets so we can use them later to make predictions and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming x_val and x_test with scalers from X_train\n",
    "X_val[to_robust_scale] = robust_scaler.transform(X_val[to_robust_scale])\n",
    "X_test[to_robust_scale] = robust_scaler.transform(X_test[to_robust_scale])\n",
    "\n",
    "X_val[to_standardize] = stand_scaler.transform(X_val[to_standardize])\n",
    "X_test[to_standardize] = stand_scaler.transform(X_test[to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking transformed datasets\n",
    "# X_val.head() # Uncomment to view dataframes\n",
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-cabin",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-commission",
   "metadata": {},
   "source": [
    "Let's start with a baseline model.\n",
    "\n",
    "A baseline model is a good pratice to determine if all sweat put into modelling with different algorithms and hyperparameter tuning is worth the effort.\n",
    "\n",
    "We can use a simple **Linear Regression**, not tunned, model as our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-tutorial",
   "metadata": {},
   "source": [
    "### Simple LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "# Fitting the model\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting using X_val\n",
    "y_val_pred = log_model.predict(X_val)\n",
    "# y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating metrics with Skelearn Classification Report\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n",
    "plot_roc_curve(log_model, X_val, y_val, ax = ax[0])\n",
    "plot_confusion_matrix(log_model, X_val, y_val, cmap=plt.cm.Reds, ax = ax[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-hacker",
   "metadata": {},
   "source": [
    "### Keeping metrics logs in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "EXPERIMENT_NAME = '[v2.2] [customers_whole] [Consumer Behavior Analytics] [Renan Moises]'\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_id = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "metrics = [accuracy_score, precision_score, recall_score, f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.create_run(experiment_id)\n",
    "\n",
    "client.log_param(run.info.run_id, 'model', 'LogistRegression-Baseline')\n",
    "\n",
    "for metric in zip(metrics_names, metrics):\n",
    "    client.log_metric(run.info.run_id, metric[0], metric[1](y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-color",
   "metadata": {},
   "source": [
    "## Modelling for Real\n",
    "\n",
    "- LogisticRegression (tunned)\n",
    "- KNNClassifier\n",
    "- SVC\n",
    "- RFClassifier\n",
    "- AdaBoost\n",
    "- GradientBoostClassifier\n",
    "- XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression-Tunned': LogisticRegression(),\n",
    "    'KNNClassifier': KNeighborsClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'XGboostClassifier': XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key, model_value in models.items():\n",
    "    X_train_modelling = X_train.copy().drop(to_drop_from_vif, axis = 1)\n",
    "    X_val_modelling = X_val.copy().drop(to_drop_from_vif, axis = 1)\n",
    "    \n",
    "    if model_key == 'LogisticRegression-Tunned':\n",
    "        # Dropping Multicolinear Features for Logistic Regression\n",
    "#         X_train_modelling = X_train.drop(to_drop_from_vif, axis = 1)\n",
    "#         X_val_modelling = X_val.drop(to_drop_from_vif, axis = 1)\n",
    "        param_grid = {\n",
    "            'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'tol': stats.loguniform(0.1, 1),\n",
    "            'C': stats.loguniform(3, 10)\n",
    "        }\n",
    "    \n",
    "    elif model_key == 'KNNClassifier':\n",
    "        # Dropping Multicolinear Features for KNNClassifier\n",
    "#         X_train_modelling = X_train.drop(to_drop_from_vif, axis = 1)\n",
    "#         X_val_modelling = X_val.drop(to_drop_from_vif, axis = 1)\n",
    "        param_grid = {'n_neighbors':[3, 4, 5, 6, 7]}\n",
    "    \n",
    "    elif model_key == 'SVC':\n",
    "        # Dropping Multicolinear Features for SVC\n",
    "#         X_train_modelling = X_train.drop(to_drop_from_vif, axis = 1)\n",
    "#         X_val_modelling = X_val.drop(to_drop_from_vif, axis = 1)\n",
    "        param_grid = {\n",
    "            'C': stats.loguniform(3, 10)\n",
    "        }\n",
    "    \n",
    "    elif model_key == 'RandomForestClassifier':\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 60, 70, 80, 90, 100],\n",
    "            'max_depth': np.arange(0, 10),\n",
    "            'min_samples_split': np.arange(0, 10),\n",
    "            'min_samples_leaf': stats.loguniform(.01, 1),\n",
    "        }\n",
    "    \n",
    "    elif model_key == 'AdaBoostClassifier':\n",
    "        param_grid = {\n",
    "            'learning_rate': stats.lognorm(.001, 1)\n",
    "        }\n",
    "    \n",
    "    elif model_key == 'GradientBoostingClassifier':\n",
    "        param_grid = {\n",
    "            'learning_rate': stats.lognorm(.001, 1),\n",
    "            'n_estimators': [50, 60, 70, 80, 90, 100],\n",
    "            'min_samples_split': np.arange(0, 10),\n",
    "            'min_samples_leaf': stats.loguniform(.01, 1),\n",
    "            'max_depth': np.arange(0, 10)\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 60, 70, 80, 90, 100],\n",
    "            'max_depth': np.arange(0, 10),\n",
    "            'learning_rate': stats.lognorm(.001, 1),\n",
    "            'gamma': stats.lognorm(.001, 1),\n",
    "        }   \n",
    "    \n",
    "    \n",
    "    # Running RandomizedSearchCV\n",
    "    print(model_key, '#'.replace('#', '#'*(61 - len(model_key))))\n",
    "    model_rsearch = RandomizedSearchCV(model_value, \n",
    "                                       param_distributions = param_grid, \n",
    "                                       n_iter = 50, \n",
    "                                       scoring = 'f1', # Used to update weights\n",
    "                                       cv = 10, \n",
    "                                       n_jobs = -1, \n",
    "                                       verbose = 1)\n",
    "    \n",
    "    # Fitting the model to the train data\n",
    "    model_rsearch.fit(X_train_modelling, y_train)\n",
    "    \n",
    "    # Saving model as a joblib file\n",
    "    joblib.dump(model_rsearch, f'../models/{model_key}.joblib')\n",
    "    \n",
    "    # Predictions using X_val\n",
    "    y_val_pred = model_rsearch.predict(X_val_modelling)\n",
    "    \n",
    "    # Setting up metrics\n",
    "    metrics_names = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
    "    \n",
    "    # MLFlow Logs\n",
    "    run = client.create_run(experiment_id)\n",
    "    for metric_name, metric in zip(metrics_names, metrics):\n",
    "        client.log_metric(run.info.run_id, metric_name, metric(y_val, y_val_pred))\n",
    "    client.log_param(run.info.run_id, \"model\", model_key)\n",
    "    client.log_param(run.info.run_id, \"params\", model_value.get_params())\n",
    "    client.log_param(run.info.run_id, \"features\", model_rsearch.columns.tolist())\n",
    "    \n",
    "    print(classification_report(y_val, y_val_pred), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-mayor",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-receipt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
